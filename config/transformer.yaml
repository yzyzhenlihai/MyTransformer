model:
  transformer:
    _target_: src.transformer.models.transformer.Transformer
    src_vocab_size: 5000
    tgt_vocab_size: 6000
    d_model: 128
    num_heads: 4
    d_ff: 512
    num_layers: 2
    pad_idx: 0
    dropout_attn: 0.1
    dropout_posffn: 0.1
    dropout_emb: 0.1
    max_len: 5000

# --- 训练配置 ---
training:
  epoches: 10
  batch_size: 32
  learning_rate: 3e-4
  device: "cuda:0"
  seed: 42
  checkpoint_dir: "./checkpoints"
  results_dir: "./results"
  open_swanlab: true
  swanlab_key: "ccVCViGdYDi4LOGAy6FBp"
  # 定义优化器
  optimizer:
    _target_: torch.optim.Adam
    lr: ${training.learning_rate}  # 学习率由调度器管理
    betas: [0.9, 0.98]
    eps: 1e-9
  # 定义调度器
  scheduler:
    _target_: training.scheduler.NoamLR
    warmup_steps: 4000
    d_model: ${model.transformer.d_model}

# 数据配置
data:
  scripts_path: "./data/iwslt2017"
  data_name: "iwslt2017-en-de"
  data_dir: "./data/iwslt2017"
  lang_src: "en"
  lang_tgt: "de"
  src_tokenizer_path: "tokenizer_src_${data.lang_src}.json"
  tgt_tokenizer_path: "tokenizer_tgt_${data.lang_tgt}.json"

# 特殊标记
tokens:
  pad_token: "[PAD]"
  unk_token: "[UNK]"
  bos_token: "[BOS]"
  eos_token: "[EOS]"

  pad_idx: 0
  unk_idx: 1
  bos_idx: 2
  eos_idx: 3

